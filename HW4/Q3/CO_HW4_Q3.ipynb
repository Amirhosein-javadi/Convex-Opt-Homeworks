{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Distributions\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "N = 200\n",
    "x = rand(Uniform(-1,1), (200,2))\n",
    "y = x[:,1] .* x[:,2];\n",
    "theta = rand(Uniform(-1,1), (13,1))\n",
    "New_theta = theta\n",
    "Kmax = 150;\n",
    "lambda = 1e-5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_matrix (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Sigmoid(x)\n",
    "   return (exp.(x) .- exp.(-x)) ./ (exp.(x) .+ exp.(-x))\n",
    "end\n",
    "function D_Sigmoid(x)\n",
    "   return 4 ./ ((exp.(x) .+ exp.(-x)).^2)\n",
    "end\n",
    "function RMS_loss(x,y,theta)\n",
    "   return (norm(f(theta, x).-y)^2)/N \n",
    "end\n",
    "function f(theta, x)\n",
    "    return theta[1].*Sigmoid(theta[2].*x[:,1].+theta[3].*x[:,2].+theta[4]) .+\n",
    "           theta[5].*Sigmoid(theta[6].*x[:,1].+theta[7].*x[:,2].+theta[8]) .+\n",
    "           theta[9].*Sigmoid(theta[10].*x[:,1].+theta[11].*x[:,2].+theta[12]) .+ theta[13]\n",
    "end\n",
    "function derivative_matrix(theta, x)\n",
    "    Dr = zeros((200,13))\n",
    "    Dr[:,1] = Sigmoid(theta[2].*x[:,1].+theta[3].*x[:,2].+theta[4])\n",
    "    Dr[:,2] = theta[1].*x[:,1].*D_Sigmoid(theta[2].*x[:,1].+theta[3].*x[:,2].+theta[4])\n",
    "    Dr[:,3] = theta[1].*x[:,2].*D_Sigmoid(theta[2].*x[:,1].+theta[3].*x[:,2].+theta[4])\n",
    "    Dr[:,4] = theta[1].*D_Sigmoid(theta[2].*x[:,1].+theta[3].*x[:,2].+theta[4])\n",
    "    Dr[:,5] = Sigmoid(theta[6].*x[:,1].+theta[7].*x[:,2].+theta[8])\n",
    "    Dr[:,6] = theta[5].*x[:,1].*D_Sigmoid(theta[6].*x[:,1].+theta[7].*x[:,2].+theta[8])\n",
    "    Dr[:,7] = theta[5].*x[:,2].*D_Sigmoid(theta[6].*x[:,1].+theta[7].*x[:,2].+theta[8])\n",
    "    Dr[:,8] = theta[5].*D_Sigmoid(theta[6].*x[:,1].+theta[7].*x[:,2].+theta[8])\n",
    "    Dr[:,9] = Sigmoid(theta[10].*x[:,1].+theta[11].*x[:,2].+theta[12])\n",
    "    Dr[:,10] = theta[10].*x[:,1].*D_Sigmoid(theta[10].*x[:,1].+theta[11].*x[:,2].+theta[12])\n",
    "    Dr[:,11] = theta[10].*x[:,2].*D_Sigmoid(theta[10].*x[:,1].+theta[11].*x[:,2].+theta[12])\n",
    "    Dr[:,12] = theta[10].*D_Sigmoid(theta[10].*x[:,1].+theta[11].*x[:,2].+theta[12])\n",
    "    Dr[:,13] .= 1\n",
    "    return Dr\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_value = zeros(Kmax);\n",
    "gradient_norm = zeros(Kmax);\n",
    "New_theta = theta\n",
    "for i =1:Kmax\n",
    "    D = derivative_matrix(theta, x)\n",
    "    New_theta = (transpose(D) * D + lambda * I) \\ (transpose(D)*(D*theta.+y.-f(theta, x)))\n",
    "    if RMS_loss(x,y,New_theta) < RMS_loss(x,y,theta)\n",
    "        theta = New_theta\n",
    "        lambda *= 0.8\n",
    "    else\n",
    "        lambda *= 2\n",
    "    end\n",
    "    f_value[i] = (RMS_loss(x,y,theta)^0.5)*N\n",
    "    gradient_norm[i] = norm(derivative_matrix(theta, x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr();\n",
    "plot(1:Kmax, f_value,\n",
    "    xlabel = \"Iteration\",\n",
    "    ylabel = \"loss\",\n",
    "    title = \"LM Loss vs Iterations\",\n",
    "    legend = false,\n",
    "    grid = true)\n",
    "savefig(\"LM_Loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:Kmax, gradient_norm,\n",
    "    xlabel = \"Iteration\",\n",
    "    ylabel = \"Norm of Gradient\",\n",
    "    title = \"Norm of Gradient vs Iterations\",\n",
    "    legend = false,\n",
    "    grid = true)\n",
    "savefig(\"Norm_of_Gradient.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMS fitting error of linear model is 0.3372979516658594\n",
      "The RMS fitting error of neural network is 0.3401538124992574\n"
     ]
    }
   ],
   "source": [
    "data = hcat(x, ones(200,1)) \n",
    "Model = data \\ y\n",
    "Linear_Model_RMS = norm(data*Model-y)/(N^0.5)\n",
    "println(\"The RMS fitting error of linear model is \", string(Linear_Model_RMS))\n",
    "Neural_Network_RMS = norm(f(theta, x).-y)/(N^0.5)\n",
    "println(\"The RMS fitting error of neural network is \", string(Neural_Network_RMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  0.06593416795037406\n",
       " -0.038839666778029074\n",
       "  0.006736608984711866"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
